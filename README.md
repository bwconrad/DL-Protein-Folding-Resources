# Protein Folding Resources

## Protein Folding Models
- [Alphafold 1](https://www.nature.com/articles/s41586-019-1923-7)
    - [DeepMind Blog Post](https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery)
    - [Andrew Senior's Talk](https://www.youtube.com/watch?v=uQ1uVbrIv-Q)
- Alphafold 2
    - [DeepMind Blog Post](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)
    - [CASP14 Presentation](https://xukui.cn/alphafold2.html)
    - [Yannic Kilcher's Video Overview](https://www.youtube.com/watch?v=B9PL__gVxLI)
    - [Fabian Fuchs' Blog Post](https://fabianfuchsml.github.io/alphafold2/)
    - [Mohammed AlQuraishi's Blog Post](https://moalquraishi.wordpress.com/2020/12/08/alphafold2-casp14-it-feels-like-ones-child-has-left-home/)

## Equivariance
- [CNNs and Equivariance Blog Post](https://fabianfuchsml.github.io/equivariance1of2/)
- [Symmetry and Equivariance in Neural Networks Lecture](https://www.youtube.com/watch?v=8s0Ka6Y_kIM)
- [SE(3)-Transformers](https://arxiv.org/abs/2006.10503)
- [Iterative SE(3)-Transformers](https://arxiv.org/abs/2102.13419)
    - [Blog post](https://fabianfuchsml.github.io/se3iterative/)
- [Lie Transformer](https://arxiv.org/abs/2012.10885)
- [Steerable CNNs](https://arxiv.org/abs/1612.08498)
- [Tensor field networks](https://arxiv.org/abs/1802.08219)
- [E(n) Equivariant Graph Neural Networks](https://arxiv.org/abs/2102.09844)
- [Exploiting Symmetries in Inference and Learning](https://www.youtube.com/watch?v=yCRBZ5kjSmI&t=1s)

## Attention Layers
- [Nyströmformer: A Nyström-Based Algorithm for Approximating
Self-Attention](https://arxiv.org/abs/2102.03902)
    - [Yannic Kilcher's Video Overview](https://www.youtube.com/watch?v=m-zrcmRd7E4)
- [Axial Attention in Multidimensional Transformers](https://arxiv.org/abs/1912.12180)
- [Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation](https://arxiv.org/abs/2003.07853v2)

## Protein Language Models 
- [MSA Transformer](https://www.biorxiv.org/content/10.1101/2021.02.12.430858v1)
- [BERTology Meets Biology: Interpreting Attention in Protein Language Models](https://arxiv.org/abs/2006.15222) 
    - [Yannic Kilcher's Video Overview](https://www.youtube.com/watch?v=q6Kyvy1zLwQ)
- [Transformer protein language models are unsupervised structure learners](https://openreview.net/pdf?id=fylclEqgvgd)
- [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.biorxiv.org/content/10.1101/622803v4)

## Contact Prediction
[Single Layers of Attention Suffice to Predict Protein Contacts](https://www.biorxiv.org/content/10.1101/2020.12.21.423882v1?rss=1)

## Miscellaneous Transformers
- [Improve Transformer Models with Better Relative Position Embeddings](https://arxiv.org/abs/2009.13658)
